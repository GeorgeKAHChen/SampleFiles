%Terminal Command = xelatex -pdf FileName
%Pretreatment ========================================================
%\documentclass[notheorems, UTF8]{ctexbeamer}
\documentclass[notheorems, UTF8]{beamer}
\mode<presentation> {
\usetheme{Madrid}
}

\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{geometry}
\usepackage{listings} 
\usepackage{verbatim}
\usepackage{cite}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage{appendix}  
\usepackage{graphics}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{graphicx}
\usepackage[amsmath,thmmarks]{ntheorem} 
\usepackage{color}
\usepackage{wrapfig}
\usepackage[dvipsnames]{xcolor}
%\usepackage{subcaption}


\theoremstyle{plain}
\theoremseparator{\hspace{1em}} \theoremnumbering{arabic}
\theoremsymbol{}
\newtheorem{theorem}{\textbf{Theorem}}[section]
\newtheorem{definition}{\textbf{Definition}}[section]
\newtheorem{lemma}{\textbf{Lemma}}[section]
\newtheorem{property}{\textbf{Property}}[section]
\newtheorem{proof}{\textit{PROOF}}[section]
\newtheorem{example}{\textbf{E x a m p l e}}[section]
\newtheorem{problem}{\textbf{P r o b l e m}}[section]
\newtheorem{solution}{\textit{SOLUTION}}[section]
\newtheorem{discussion}{\textit{D I S C U S S I O N}}[section]
\newtheorem{conclusion}{\textit{\textbf{CONCLUSION}}}[section]

 
%Infomation ========================================================
\title[CRIEA]{CARLA - RW Image Edge Algorithm}
\author{Kazuki Amakawa}
\institute[USTB DMP]
{
University of Science and Technology Beijing, Department of Mathematics and Physicis
\medskip\\
\textit{KazukiAmakawa@gmail.com}
}
\date{\today}

%Title and menu ========================================================
\begin{document}

\begin{frame}
\titlepage
\end{frame}

%Main ========================================================
%Section ========================================================
\begin{frame}
\frametitle{统计决策理论}
1、以期望值为标准的决策方法\\
2、以最大的可能性为标准的决策方法\\
3、以意向水准原则为标准的决策方法\footnote{Reference(Chinese)\\:\texttt{$http://wiki.mbalib.com/wiki/\%E7\%BB\%9F\%E8\%AE\%A1\%E5\%86\%B3\%E7\%AD\%96\%E6\%B3\%95$}}\\
\end{frame}



\begin{frame}
\frametitle{随机游走分类器}
转移概率矩阵的定义
\begin{displaymath}
p_{ij} = \left\{
\begin{array}{ll}
1 & i = j   \ and\ v_i \in V_l \\
0 & i \neq j\ and\ v_i \in V_l \\
\omega_{ij} & Elsewhere
\end{array}
\right .
\end{displaymath}

分块转移概率矩阵
\begin{displaymath}
P_{n \times n} = \left(
\begin{array}{ll}
P_{ll} & P_{iu}\\
P_{ul} & P_{uu} 
\end{array}
\right)=\left(
\begin{array}{ll}
I_{l}  & 0_{lu}\\
P_{ul} & P_{uu} 
\end{array}
\right)
\end{displaymath}

初始状态的决策矩阵
\begin{displaymath}
D_{u \times n}(0) = \left[
\begin{array}{c}
d_1 \\
d_2 \\
\ldots \\
d_u
\end{array}
\right] = \left[
\begin{array}{ccc}
0 & \ldots & 0\\
0 & \ldots & 0\\
\ldots & \ldots & \ldots\\
0 & \ldots & 0\\
\end{array}
\left | 
\begin{array}{cccc}
1 & 0 & \ldots & 0 \\
0 & 1 & \ldots & 0 \\
\ldots & \ldots &\ldots & \ldots \\
0 & 0 & \ldots & 1 \\
\end{array}
\right]_{u \times n} = 
\left[
\begin{array}{cc}
0_{u l} & I_{u} \\
\end{array}
\right ]\right.
\end{displaymath}
迭代公式
\begin{displaymath}
D_{u \times n}(t + 1) = D_{u \times n}(t) \times P_{n \times n}
\end{displaymath}
\end{frame}




\begin{frame}
迭代公式
\begin{displaymath}
D_{u \times n}(t + 1) = D_{u \times n}(t) \times P_{n \times n}
\end{displaymath}

最终状态的迭代公式
\begin{displaymath}
D_{u \times n}(\infty) = \left[
\begin{array}{c}
d_1(\infty) \\
d_2(\infty) \\
\ldots \\
d_u(\infty) \\
\end{array}
\right] = D_{u \times n}(0) \times P_{n \times n}^{\infty}
\end{displaymath}

归纳法易证
\begin{displaymath}
P^n =  \left(
\begin{array}{ll}
I_{ l}      & 0_{l u}\\
\left(\sum_{i=1}^{n} P_{uu}^{n-1} \right) P_{ul} & P_{uu}^n
\end{array}
\right) 
\end{displaymath}

所以，如果$(I_u-P_{uu})^{-1}$可逆，则有
\begin{displaymath}
D_{u \times n}(\infty) = \left[
\begin{array}{ll}
(I_u - P_{uu})^{-1}P_{ul} & 0_{uu}
\end{array}
\right] = \left[
\begin{array}{ll}
D_{ul} & 0_{uu}
\end{array}
\right]
\end{displaymath}

最终决策矩阵有
\begin{displaymath}
D_{ul} = (I_u - P_{uu})^{-1}P_{ul}
\end{displaymath}
\end{frame}



\begin{frame}
\frametitle{化简RW方法}
定义
\begin{displaymath}
X = P^\infty =  \left(
\begin{array}{ll}
I_{l \times l}      & 0_{l \times u}\\
D_{ul} & 0_{uu}
\end{array}
\right) 
\end{displaymath}

最终化简得到
\begin{displaymath}
P_{ul} + (P_{uu}-I_u)X_{ul} = 0
\end{displaymath}

但是在实际运算中，仍然存在三个问题，其一，权值函数的选取，其二，结点数量过多爆内存，其三，权值结果过小造成运算不准确\\
第一个问题的解决方案，是套用高斯模型，即使用如下的权值公式
\begin{displaymath}
\omega_{ij} = exp(-\alpha(g_i - g_j)^2 -\beta((x_i-x_j)^2 + (y_i-y_j)^2 ))
\end{displaymath}
第二个问题的解决方案是使用Toboggan算法，接下来简单的介绍一下Toboggan算法
\end{frame}



\begin{frame}
\frametitle{Toboggan算法}
Toboggan 算法的核心在于寻找8-邻域中梯度最低的点，如果该点是当前点，那么就给这些点一个新的标签，如果该点不是当前点且没有标签，那么就以这个点为中心继续寻找。如果该点不是当前点且有标签，那么栈中的所有点都打上这个标签。不断循环，直到遍历完全图\\

作为一种边界算法，Toboggan并不合格，因为Toboggan往往会造成过拟合。但是如果将属性相近的点结合在一起的话，Toboggan算法绝对是一种很不错的算法，且时间复杂度较低。
%Figure
\end{frame}

\begin{frame}
\frametitle{权值结果过小问题}
在之前的讨论中，有
\begin{displaymath}
(I_u - P_{uu})X_{ul} = P_{ul}
\end{displaymath}

展开，有
\begin{displaymath}
P_{uu} = \left(
\begin{array}{cccc}
p_{u_1}^{u_1} & p_{u_1}^{u_2} & \ldots & p_{u_1}^{u_n}  \\
p_{u_2}^{u_1} & p_{u_2}^{u_2} & \ldots & p_{u_2}^{u_n}  \\
\ldots & \ldots & \ldots & \ldots \\
p_{u_n}^{u_1} & p_{u_n}^{u_2} & \ldots & p_{u_n}^{u_n}  \\
\end{array}
\right)
\end{displaymath}

\begin{displaymath}
P_{ul} = \left(
\begin{array}{cccc}
p_{u_1}^{l_1} & p_{u_1}^{l_2} & \ldots & p_{u_1}^{l_n}  \\
p_{u_2}^{l_1} & p_{u_2}^{l_2} & \ldots & p_{u_2}^{l_n}  \\
\ldots & \ldots & \ldots & \ldots \\
p_{u_n}^{l_1} & p_{u_n}^{l_2} & \ldots & p_{u_n}^{l_n}  \\
\end{array}
\right)
\end{displaymath}

根据权值函数的定义，我们有
\begin{displaymath}
I_u - P_{uu} = I_u - \left(
\begin{array}{cccc}
1 & p_{u_1}^{u_2} & \ldots & p_{u_1}^{u_n}  \\
p_{u_2}^{u_1} & 1 & \ldots & p_{u_2}^{u_n}  \\
\ldots & \ldots & \ldots & \ldots \\
p_{u_n}^{u_1} & p_{u_n}^{u_2} & \ldots & 1  \\
\end{array}
\right) = \left(
\begin{array}{cccc}
0 & p_{u_1}^{u_2} & \ldots & p_{u_1}^{u_n}  \\
p_{u_2}^{u_1} & 0 & \ldots & p_{u_2}^{u_n}  \\
\ldots & \ldots & \ldots & \ldots \\
p_{u_n}^{u_1} & p_{u_n}^{u_2} & \ldots & 0  \\
\end{array}
\right)
\end{displaymath}
\end{frame}



\begin{frame}
由于 $p_{ij} = \omega_{ij} / d_i$ 且 
\begin{displaymath}
d_i = \sum_{j \neq i} \omega_{ij}
\end{displaymath}

所以
\begin{displaymath}
(I_u - P_{uu}) = 1 / \prod_{i = l + 1}^{n}d_i \left(
\begin{array}{cccc}
0 & \omega_{u_1}^{u_2} & \ldots & \omega_{u_1}^{u_n}  \\
\omega_{u_2}^{u_1} & 0 & \ldots & \omega_{u_2}^{u_n}  \\
\ldots & \ldots & \ldots & \ldots \\
\omega_{u_n}^{u_1} & \omega_{u_n}^{u_2} & \ldots & 0  \\
\end{array}
\right) = \Omega_{uu}
\end{displaymath}

而另一方面
\begin{displaymath}
P_{ul} = 1 / \prod_{i = l + 1}^{n}d_i \left(
\begin{array}{cccc}
\omega_{u_1}^{l_1} & \omega_{u_1}^{l_2} & \ldots & \omega_{u_1}^{l_n}  \\
\omega_{u_2}^{l_1} & \omega_{u_2}^{l_2} & \ldots & \omega_{u_2}^{l_n}  \\
\ldots & \ldots & \ldots & \ldots \\
\omega_{u_n}^{l_1} & \omega_{u_n}^{l_2} & \ldots & \omega_{u_n}^{l_n}  \\
\end{array}
\right) = \Omega_{ul}
\end{displaymath}

最终，方程式被化简为了
\begin{displaymath}
\Omega_{uu}X_{ul} = \Omega_{ul}
\end{displaymath}
\end{frame}



\begin{frame}
\frametitle{Random Walk Image Edge Algorithm}
\begin{algorithm}[H]
\begin{algorithmic}
\State STEP 0 Using Toboggan algorithm to partial the image
\State STEP 1 Find the initial label infomation and seed pixel, remember the set $V_l$ and $V_u$
\State STEP 2 Build the weight matrix $\Omega$ with
\begin{displaymath}
\omega_{ij} = \left\{
\begin{array}{ll}
0 & if\ v_i \in V_l\\
exp\{-\alpha(g_i - g_j)^2 -\beta[(x_i-x_j)^2 + (y_i-y_j)^2 ]\}  &  i \neq j\ and\ v_i \not\in V_l\\
0 & elsewhere
\end{array}
\right .
\end{displaymath}

\State STEP 3 Solve the linear equation as 
\begin{displaymath}
\Omega_{uu}X_{ul} = \Omega_{ul}
\end{displaymath}
And get the decision matrix $D_{ul} = X_{ul}$

\State STEP 4 Make the decision with decision matrix and get the segmentation result

\end{algorithmic}
\end{algorithm}
\end{frame}




\begin{frame}
\frametitle{结论}
1) 非全自动，需要得到标签和种子结点\\
2) 初始值影响结果影响的非常大\\
\begin{figure}[H]
\centering
\subfigure[1]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/RWIEA/Aznyan.png}
\end{minipage}
}
\subfigure[2]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/RWIEA/Caman.png}
\end{minipage}
}
\caption{RWIEA 结果} \label{fig:1}
\end{figure}

\end{frame}




\begin{frame}
\frametitle{K-means 方法(基于EM)}
\begin{algorithm}[H]
\caption{K-means algorithm(EM solution method)}
\begin{algorithmic}
\State STEP 1 Initialize k cluster center randomly
\begin{displaymath}
U = \{u_1(0), u_2(0), \ldots, u_k(0)\}
\end{displaymath}
\While 1
	\State STEP 2.1 In time $t$, find the cluster label for every data point, let the label for every point is $l_i$ for $x_i$, then $l_i$ s.t.$l_i(t) = \arg\min_{u_j(t)} \|x_i - u_j(t)\|^2$
	\State STEP 2.2 For every cluster set, calculate the new center based on average of all points belonging to this cluster$u_j(t + 1) = {1\over \lambda_j(t)}\sum_{x_i \in S_j(t)} x_i$, where $\lambda_j(t) = |S_j(t)|$
\EndWhile
\State STEP 3 Return $U(n)$ as cluster center and $S(n)$ as cluster set of all data
\end{algorithmic}
\end{algorithm}
\begin{center}
怎么确定 K 的值？
\end{center}
\end{frame}





\begin{frame}
\frametitle{结论}
1) 欠拟合和过拟合问题\\
2) 每一个结点的属性只能是属于或者不属于\\
\begin{figure}[H]
\centering
\subfigure[1]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/K-means/Aznyan.png}
\end{minipage}
}
\subfigure[2]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/K-means/Caman.png}
\end{minipage}
}
\caption{RWIEA 结果} \label{fig:1}
\end{figure}
\end{frame}



\begin{frame}
\frametitle{最大似然估计}
定义式
\begin{displaymath}
\Theta_{best} = \arg \max_{\Theta} L(\Theta) = \arg \max_{\Theta} \prod_{i = 1}^{n} P(x_i; \Theta)
\end{displaymath}

等价变换
\begin{displaymath}
\Theta_{best} = \arg \max_{\Theta} {1 \over n} \sum_{i = 1}^{n} \log\left(  P(x_i; \Theta)\right) = \arg \max_{\Theta} E_{\Theta} \left( \log P(X)\right)
\end{displaymath}
\end{frame}



\begin{frame}
\frametitle{从M到EM}
Jensen不等式\\
\begin{displaymath}
E[f(x)] \geq f(E[x])
\end{displaymath}


\begin{displaymath}
\Theta_{best} = \arg \max_{\Theta} \sum_{i = 1}^{n} \log\left(  P_{\Theta}(x_i)\right)
\end{displaymath}

继续推广
\begin{displaymath}
\Theta_{best} = \arg \max_{\Theta} \sum_{i = 1}^{n} \left( \log \sum_{j = 1}^{n} P_{\Theta}(x_i^{(o)}, x_j^{(u)}) \right)\end{displaymath}
\begin{displaymath} 
= \arg \max_{\Theta} \sum_{i = 1}^{n} \left[ \log E^{(x_j{(u)})}\left( P_{\Theta}(x_i^{(o)}, x_j^{(u)}) \right)\right]
\end{displaymath}
\begin{displaymath}
\geq \arg \max_{\Theta} \sum_{i = 1}^{n} \left[ E^{(x_j{(u)})}\left( \log P_{\Theta}(x_i^{(o)}, x_j^{(u)}) \right)\right] 
\end{displaymath}
\end{frame}



\begin{frame}
\frametitle{从M到EM}
Jensen不等式\\
\begin{displaymath}
E[f(x)] \geq f(E[x])
\end{displaymath}


\begin{displaymath}
\Theta_{best} = \arg \max_{\Theta} \sum_{i = 1}^{n} \log\left(  P_{\Theta}(x_i)\right)
\end{displaymath}

继续推广
\begin{displaymath}
\Theta_{best} \geq \arg \max_{\Theta} \sum_{i = 1}^{n} \left[ \int_{\Omega}\left( \log P_{\Theta}(x_i^{(o)}, X^{(u)}) Q_i(X^{(u)}) \right) d X{(u)} \right] 
\end{displaymath}
其中$Q_i(X^{(u)})$为$X^{(u)}$的分布函数，满足
\begin{displaymath}
p_{\Theta^{(t)}}(X^{(u)} | x_i^{(o)}) = Q_i^t(X^{(u)}) = {p_{\Theta^{(t)}}(x_i^{(o)}, X^{(u)}) \over \int_{\Omega}p_{\Theta^{(t)}}(x_i^{(o)}, X^{(u)}) d X^{(u)}}
\end{displaymath} 
\end{frame}




\begin{frame}
\frametitle{结论}
1) GMM-EM 没有考虑到空间位置的问题，在有噪音的图像中不能很好的得到结果\\
2) EM方法有时会陷入局部最优\\

解决方法：\\
1) 使用基于模拟退火的DAEM方法\\
2) 使用CARLA方法\\
实际上，DAEM和CARLA的原理都是Monte Carlo
\end{frame}



\begin{frame}

\begin{algorithm}[H]
\tiny
\begin{algorithmic}
\State 1) determine the consume function $J(x, i)$
\State 2) Initialization PDF function as 
\begin{displaymath}
f(\tau, 0) = {1 \over x_{max} - x_{min}}\ \ \ while\  x_{min} \leq x \leq x_{max}
\end{displaymath}
\While {$i \leq Total\ Loop$}
\State 3.1) Get a random number $z = random() \in [0, 1]$
\State 3.2) Find a number $x_i$, which s.t.
\begin{displaymath}
\int_{x_{min}}^{x} f(\tau, i) d\tau = z
\end{displaymath}
\State 3.3) Calculate the cost $J_i = J(x_i, i)$
\State 3.4) Refresh $J_{med}, J_{min}$ include new value $J_i$ in the set of $R$
\State 3.5) Calculate the reinforcement parameter with
\begin{displaymath}
\beta_{i+1} = max\left\{0, {J_{med} - J_i \over J_{med} - J_{min}}\right\}
\end{displaymath}
\State 3.6) Calculate $\alpha_{i+1}$ and get the new PDF for next loop
\begin{displaymath}
\alpha_{i+1} = \left\{1 + \beta_{i+1} {\sqrt{2\pi} \over 2} \lambda \sigma \left[erf\left({x_{max} - x_i \over \sqrt{2} \sigma} \right) - erf\left({x_{min} - x_i \over \sqrt{2} \sigma} \right)\right]\right\}
\end{displaymath}
\State 3.7) Let i = i + 1, continue
\EndWhile
\State 4) Calculate the expectation of PDF $f(\tau, n)$ as 
\begin{displaymath}
E(\tau) = \int_{x_{min}}^{x_{max}} \tau f(\tau, n) d\tau
\end{displaymath}
and return $E(\tau)$ as decision value.
\end{algorithmic}
\end{algorithm}
\end{frame}



\begin{frame}
\frametitle{在参数学习中}
\begin{figure}[H]
\centering
\subfigure[参数学习-PDF变化]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/200times.png}
\end{minipage}
}
\subfigure[参数学习-X变化]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/VarX.png}
\end{minipage}
}
\subfigure[结果]{
\begin{minipage}[b]{0.5\textwidth}
\includegraphics[width=1\textwidth]{Figure/His.png}
\end{minipage}
}
\caption{RWIEA 结果} \label{fig:1}
\end{figure}
\end{frame}


\begin{frame}
\frametitle{在参数学习中}
\begin{figure}[H]
\centering
\begin{minipage}[b]{0.9\textwidth}
\includegraphics[width=1\textwidth]{Figure/His.png}
\end{minipage}
\caption{RWIEA 结果} \label{fig:1}
\end{figure}
\end{frame}



\begin{frame}
\frametitle{结论}
1) 仍然没有考虑到距离问题，即噪音出现怎么办？\\
\begin{figure}[H]
\subfigure[1]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/CARLA/Aznyan.png}
\end{minipage}
}
\subfigure[2]{
\begin{minipage}[b]{0.46\textwidth}
\includegraphics[width=1\textwidth]{Figure/CARLA/Factory.png}
\end{minipage}
}
\caption{RWIEA 结果} \label{fig:1}
\end{figure}
\end{frame}



\begin{frame}
\begin{figure}[H]
\subfigure[1]{
\begin{minipage}[b]{0.49\textwidth}
\includegraphics[width=1\textwidth]{Figure/oup1.png}
\end{minipage}
}
\subfigure[2]{
\begin{minipage}[b]{0.49\textwidth}
\includegraphics[width=1\textwidth]{Figure/oup2.png}
\end{minipage}
}
\caption{使用CARLA=GMM模型，分块后的结点间距离情况} \label{fig:1}
\end{figure}
\end{frame}





\begin{frame}
\begin{figure}[H]
\subfigure[Initial]{
\begin{minipage}[b]{0.42\textwidth}
\includegraphics[width=1\textwidth]{Figure/initial/Aznyan.png}
\end{minipage}
}
\subfigure[RWIEA]{
\begin{minipage}[b]{0.42\textwidth}
\includegraphics[width=1\textwidth]{Figure/RWIEA/Aznyan.png}
\end{minipage}
}
\subfigure[K-means]{
\begin{minipage}[b]{0.42\textwidth}
\includegraphics[width=1\textwidth]{Figure/K-means/Aznyan.png}
\end{minipage}
}
\subfigure[CARLA]{
\begin{minipage}[b]{0.42\textwidth}
\includegraphics[width=1\textwidth]{Figure/CARLA/Aznyan.png}
\end{minipage}
}
\caption{Aznyan 结果} \label{fig:1}
\end{figure}
\end{frame}


\end{document}



















